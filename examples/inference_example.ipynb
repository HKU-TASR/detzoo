{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujialin/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from detzoo.models import YOLOv1\n",
    "from detzoo.datasets import VOCDataset, COCODataset\n",
    "from detzoo.utils import collate_fn, plot_image_and_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = 'voc'\n",
    "model = 'yolov1'\n",
    "backbone = 'vgg16'\n",
    "checkpoint_path = f'/home/lujialin/detzoo/checkpoints/{model}_{backbone}_{dataset}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'voc':\n",
    "    test_dataset = VOCDataset(\n",
    "                root='~/data/VOC', \n",
    "                year='2007', \n",
    "                image_set='val', \n",
    "                transform=T.Compose(\n",
    "                    [T.Resize((448, 448)),\n",
    "                    T.ToTensor(),\n",
    "                    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    "                ),\n",
    "                # classes=classes\n",
    "            )\n",
    "elif dataset == 'coco':\n",
    "    test_dataset = COCODataset(\n",
    "                root='~/data/COCO', \n",
    "                year='2017', \n",
    "                image_set='val', \n",
    "                transform=T.Compose(\n",
    "                    [T.Resize((448, 448)),\n",
    "                    T.ToTensor(),\n",
    "                    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    "                ),\n",
    "                # classes=classes\n",
    "            )\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if model == 'yolov1':\n",
    "    detector = YOLOv1(\n",
    "                    classes=test_dataset.classes, \n",
    "                    backbone=backbone\n",
    "                ).to(device)\n",
    "elif model == 'yolov2':\n",
    "    pass\n",
    "\n",
    "detector.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n",
      "No object detected in the image.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m image \u001b[38;5;241m=\u001b[39m std \u001b[38;5;241m*\u001b[39m image \u001b[38;5;241m+\u001b[39m mean\n\u001b[1;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(image, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m plot_image_and_boxes(image[\u001b[38;5;241m0\u001b[39m], \u001b[43mdetection_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, test_dataset\u001b[38;5;241m.\u001b[39mclasses)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "detector.eval()\n",
    "\n",
    "image, _ = next(iter(test_dataloader))\n",
    "image = image.to(device)\n",
    "detection_result = detector.run(image)\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "image = image.permute(0, 2, 3, 1).cpu().numpy()\n",
    "image = std * image + mean\n",
    "image = np.clip(image, 0, 1)\n",
    "\n",
    "plot_image_and_boxes(image[0], detection_result[0], test_dataset.classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
